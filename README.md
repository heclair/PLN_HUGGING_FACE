# RAG + Hugging Face (Aula 3)

Implementa√ß√£o completa de um **chatbot com RAG** (Retrieval-Augmented Generation) usando **FastAPI**, **FAISS / embeddings**, **Hugging Face (remoto ou local)** e um **frontend em Streamlit** com UI de ingest√£o e chat.

## ‚ú® O que o projeto oferece

- **API FastAPI** com:
  - Ingest√£o de textos e arquivos `.txt` (com **chunking opcional**)
  - Indexa√ß√£o vetorial (FAISS/in-memory) via **Sentence-Transformers**
  - **RAG**: recupera√ß√£o dos k documentos mais similares + chamada ao LLM
  - **Chat** com hist√≥rico + **prompt engineering** b√°sico
  - Rotas de diagn√≥stico: health, config e teste do LLM
- **Fallback local** do LLM (Transformers) caso a Inference API n√£o esteja dispon√≠vel
- **Frontend Streamlit**:
  - Coluna esquerda: **ingest√£o** com cards dos trechos rec√©m-ingeridos
  - Coluna direita: **chat** com bolhas, **chips de fontes**, presets e ‚Äútons‚Äù (conciso/explicativo/infantil)
- Pronto para separar **backend** e **frontend** (cada um com seu `.env` e `requirements.txt`)

---

## üß± Stack

- **Backend**: Python, FastAPI, Uvicorn, FAISS (ou in-memory), Sentence-Transformers
- **LLM (remoto)**: Hugging Face **Inference API**
- **LLM (local)**: `transformers` + modelo leve (ex.: `google/flan-t5-small`)
- **Frontend**: Streamlit

---

## üìÅ Estrutura (resumo)

```
.
‚îú‚îÄ app/
‚îÇ  ‚îú‚îÄ main.py
‚îÇ  ‚îú‚îÄ routes/
‚îÇ  ‚îÇ  ‚îú‚îÄ health.py
‚îÇ  ‚îÇ  ‚îú‚îÄ ingest.py
‚îÇ  ‚îÇ  ‚îú‚îÄ query.py
‚îÇ  ‚îÇ  ‚îî‚îÄ chat.py
‚îÇ  ‚îú‚îÄ services/
‚îÇ  ‚îÇ  ‚îú‚îÄ bootstrap.py
‚îÇ  ‚îÇ  ‚îú‚îÄ embeddings.py
‚îÇ  ‚îÇ  ‚îú‚îÄ index.py
‚îÇ  ‚îÇ  ‚îî‚îÄ rag.py
‚îÇ  ‚îú‚îÄ core/
‚îÇ  ‚îÇ  ‚îú‚îÄ config.py
‚îÇ  ‚îÇ  ‚îî‚îÄ llm.py
‚îÇ  ‚îî‚îÄ models/
‚îÇ     ‚îî‚îÄ schemas.py
‚îú‚îÄ frontend/
‚îÇ  ‚îî‚îÄ app.py
‚îú‚îÄ data/                 # (opcional) √≠ndice persistido
‚îú‚îÄ .env                  # backend
‚îú‚îÄ frontend/.env         # frontend
‚îî‚îÄ requirements.txt
```

---

## ‚öôÔ∏è Configura√ß√£o (Backend)

### 1) Pr√©-requisitos
- Python **3.10+** (recomendado 3.10‚Äì3.12)
- Windows: PowerShell ou CMD; Linux/macOS: bash/zsh

### 2) Ambiente virtual e depend√™ncias
```bash
# Linux/macOS
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

```powershell
# Windows (PowerShell)
python -m venv .venv
.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

### 3) `.env` (backend) ‚Äî exemplo
> **N√£o** comitar o `.env`.

```env
# HF remoto (opcional)
HF_TOKEN=hf_xxx_sua_chave
HF_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Embeddings (PT): robusto a min√∫sculas/acentos
EMBED_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# Fallback local (recomendado manter habilitado)
HF_USE_LOCAL=1
LOCAL_MODEL=google/flan-t5-small

# Persist√™ncia/seed do √≠ndice
INDEX_DIR=data
PERSIST_INDEX=1
AUTO_SEED=1
```

> Se quiser **apenas local**, √© suficiente `HF_USE_LOCAL=1` e `LOCAL_MODEL=google/flan-t5-small` (ou outro leve).

### 4) Subir o backend
```bash
uvicorn app.main:app --reload --port 8000
```
- Swagger: http://127.0.0.1:8000/docs  
- Health: `GET /health`

---

## üñ•Ô∏è Configura√ß√£o (Frontend)

### 1) Ambiente e depend√™ncias
No diret√≥rio `frontend/`:

```bash
# Linux/macOS
python -m venv .venv
source .venv/bin/activate
pip install streamlit python-dotenv requests
```

```powershell
# Windows
python -m venv .venv
.venv\Scripts\Activate.ps1
pip install streamlit python-dotenv requests
```

### 2) `frontend/.env` (exemplo)
```env
BACKEND_URL=http://127.0.0.1:8000
CHAT_DEFAULT_TOP_K=3
CHAT_DEFAULT_TEMPERATURE=0.2
CHAT_DEFAULT_MAX_NEW_TOKENS=120
```

### 3) Rodar o front
```bash
streamlit run frontend/app.py
```

---

## üîå API ‚Äî Vis√£o geral das rotas

### Sa√∫de & Debug
- `GET /health` ‚Üí `{"status":"ok", "docs": <int>, "index_built": <bool>}`
- `GET /debug/config` ‚Üí mostra o que a API carregou do `.env` (oculta o token)
- `GET /debug/hf` ‚Üí **teste do LLM atual** (respeita fallback/local)
- `GET /debug/hf-remote` ‚Üí for√ßa **Inference API** (sem fallback) ‚Äî √∫til para diagnosticar

### Ingest√£o
- `POST /ingest/sample`  
  Ingesta 3 textos de exemplo.
- `POST /ingest/texts`  
  **Body**:
  ```json
  { "texts": ["texto 1", "texto 2"], "chunk": true }
  ```
- `POST /ingest/file`  
  **Multipart**: `file=<.txt>`, `chunk=true|false`

### RAG ‚Äúsimples‚Äù
- `POST /query`  
  **Body**:
  ```json
  {
    "question": "O que √© RAG?",
    "top_k": 3,
    "temperature": 0.2,
    "max_new_tokens": 128
  }
  ```

### Chat (com hist√≥rico)
- `POST /chat`  
  **Body**:
  ```json
  {
    "session_id": "demo",
    "message": "Explique RAG em 1 frase.",
    "top_k": 3,
    "temperature": 0.2,
    "max_new_tokens": 120,
    "system_prompt": "opcional (muda o tom da resposta)"
  }
  ```
  **Resposta**: `answer`, `sources` (ids dos docs usados), `meta` (metadados), `debug.prompt`.

> No frontend, os **chips** exibem as fontes: `Doc 0`, `Doc 1`, ...

---

## üß† Embeddings e relev√¢ncia (PT-BR)

- Embeddings definidos em `.env` via `EMBED_MODEL`.  
  Recomendado:  
  `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- O servi√ßo normaliza os textos (casefold + acentos), reduzindo sensibilidade a **min√∫sculas/mai√∫sculas** (ex.: ‚Äúbrasil‚Äù vs ‚ÄúBrasil‚Äù).
- O limiar de similaridade (`MIN_SIM`, em `rag.py`) foi ajustado para PT.  
  Se notar respostas ‚Äún√£o sei‚Äù com textos parecidos, **reduza um pouco** esse limiar.

---

## üåê Por que a Inference API do Hugging Face n√£o funcionou

Durante os testes, as chamadas para:
```
POST https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct
```
retornaram **404 Not Found**, mesmo com:
- Token v√°lido (`whoami-v2` OK)
- Model ID correto
- License aceita / acesso ao reposit√≥rio

**Causas t√≠picas**:

1) **Model ID vs disponibilidade na Serverless API**  
   Alguns reposit√≥rios (sobretudo **gated**, como Llama 3.1) n√£o est√£o habilitados na **Serverless Inference API** p√∫blica; o metadata responde, mas a rota de infer√™ncia retorna **404**.

2) **Licen√ßa ‚Äúgated‚Äù**  
   Aceitar a licen√ßa d√° acesso ao **repo**, mas n√£o implica acesso √† **serverless**. Para infer√™ncia gerenciada, use **Inference Endpoints** (pago) ou rode localmente.

3) **Capacidade/regi√£o**  
   Em certos momentos a serverless pode n√£o servir modelos maiores, devolvendo 404.

4) **Erros de token/headers** nas tentativas manuais (menos prov√°vel no backend).

**Contorno**:  
- Usar o **fallback local** (`HF_USE_LOCAL=1`) ‚Äî j√° implementado.  
- Testar com modelos **p√∫blicos** da serverless (ex.: `HuggingFaceH4/zephyr-7b-beta`, `google/flan-t5-small`).  
- Para Llama 3.1, preferir **Inference Endpoints** ou **self-host** (Transformers/TGI).

---

## ‚ñ∂Ô∏è Fluxo de uso

1. Suba o backend:
   ```bash
   uvicorn app.main:app --reload --port 8000
   ```
2. Suba o frontend:
   ```bash
   streamlit run frontend/app.py
   ```
3. Ingerir exemplos/textos e perguntar no chat.  
   As respostas citam as **fontes** (Doc X, Doc Y).

---

## üîê Boas pr√°ticas

- Nunca comite `.env` (token HF).  
- Se expor publicamente, configure **auth/rate-limit/CORS**.

---

## üìå Roadmap (sugest√µes)

- Listagem/persist√™ncia de documentos com metadados
- Hist√≥rico por `session_id`
- Suporte a E5 + re-ranker
- Streaming de tokens no chat
