# RAG + Hugging Face (Aula 3)

ImplementaÃ§Ã£o completa de um **chatbot com RAG** (Retrieval-Augmented Generation) usando **FastAPI**, **FAISS / embeddings**, **Hugging Face (remoto ou local)** e um **frontend em Streamlit** com UI de ingestÃ£o e chat.

## âœ¨ O que o projeto oferece

- **API FastAPI** com:
  - IngestÃ£o de textos e arquivos `.txt` (com **chunking opcional**)
  - IndexaÃ§Ã£o vetorial (FAISS/in-memory) via **Sentence-Transformers**
  - **RAG**: recuperaÃ§Ã£o dos k documentos mais similares + chamada ao LLM
  - **Chat** com histÃ³rico + **prompt engineering** bÃ¡sico
  - Rotas de diagnÃ³stico: health, config e teste do LLM
- **Fallback local** do LLM (Transformers) caso a Inference API nÃ£o esteja disponÃ­vel
- **Frontend Streamlit**:
  - Coluna esquerda: **ingestÃ£o** com cards dos trechos recÃ©m-ingeridos
  - Coluna direita: **chat** com bolhas, **chips de fontes**, presets e â€œtonsâ€ (conciso/explicativo/infantil)
- Pronto para separar **backend** e **frontend** (cada um com seu `.env` e `requirements.txt`)

---

## ğŸŒ Por que a Inference API do Hugging Face nÃ£o funcionou

Durante os testes, as chamadas para:
```
POST https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct
```
retornaram **404 Not Found**, mesmo com:
- Token vÃ¡lido (`whoami-v2` OK)
- Model ID correto
- License aceita / acesso ao repositÃ³rio

**Causas tÃ­picas**:

1) **Model ID vs disponibilidade na Serverless API**  
   Alguns repositÃ³rios (sobretudo **gated**, como Llama 3.1) nÃ£o estÃ£o habilitados na **Serverless Inference API** pÃºblica; o metadata responde, mas a rota de inferÃªncia retorna **404**.

2) **LicenÃ§a â€œgatedâ€**  
   Aceitar a licenÃ§a dÃ¡ acesso ao **repo**, mas nÃ£o implica acesso Ã  **serverless**. Para inferÃªncia gerenciada, use **Inference Endpoints** (pago) ou rode localmente.

3) **Capacidade/regiÃ£o**  
   Em certos momentos a serverless pode nÃ£o servir modelos maiores, devolvendo 404.

4) **Erros de token/headers** nas tentativas manuais (menos provÃ¡vel no backend).

**Contorno**:  
- Usar o **fallback local** (`HF_USE_LOCAL=1`) â€” jÃ¡ implementado.  
- Testar com modelos **pÃºblicos** da serverless (ex.: `HuggingFaceH4/zephyr-7b-beta`, `google/flan-t5-small`).  
- Para Llama 3.1, preferir **Inference Endpoints** ou **self-host** (Transformers/TGI).

---

## ğŸ§± Stack

- **Backend**: Python, FastAPI, Uvicorn, FAISS (ou in-memory), Sentence-Transformers
- **LLM (remoto)**: Hugging Face **Inference API**
- **LLM (local)**: `transformers` + modelo leve (ex.: `google/flan-t5-small`)
- **Frontend**: Streamlit

---

## ğŸ“ Estrutura (resumo)

```
.
â”œâ”€ app/
â”‚  â”œâ”€ main.py
â”‚  â”œâ”€ routes/
â”‚  â”‚  â”œâ”€ health.py
â”‚  â”‚  â”œâ”€ ingest.py
â”‚  â”‚  â”œâ”€ query.py
â”‚  â”‚  â””â”€ chat.py
â”‚  â”œâ”€ services/
â”‚  â”‚  â”œâ”€ bootstrap.py
â”‚  â”‚  â”œâ”€ embeddings.py
â”‚  â”‚  â”œâ”€ index.py
â”‚  â”‚  â””â”€ rag.py
â”‚  â”œâ”€ core/
â”‚  â”‚  â”œâ”€ config.py
â”‚  â”‚  â””â”€ llm.py
â”‚  â””â”€ models/
â”‚     â””â”€ schemas.py
â”œâ”€ frontend/
â”‚  â””â”€ app.py
â”œâ”€ data/                 # (opcional) Ã­ndice persistido
â”œâ”€ .env                  # backend
â”œâ”€ frontend/.env         # frontend
â””â”€ requirements.txt
```

---

## âš™ï¸ ConfiguraÃ§Ã£o (Backend)

### 1) PrÃ©-requisitos
- Python **3.10+** (recomendado 3.10â€“3.12)
- Windows: PowerShell ou CMD; Linux/macOS: bash/zsh

### 2) Ambiente virtual e dependÃªncias
```bash
# Linux/macOS
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

```powershell
# Windows (PowerShell)
python -m venv .venv
.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

### 3) `.env` (backend) â€” exemplo
> **NÃ£o** comitar o `.env`.

```env
# HF remoto (opcional)
HF_TOKEN=hf_xxx_sua_chave
HF_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Embeddings (PT): robusto a minÃºsculas/acentos
EMBED_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# Fallback local (recomendado manter habilitado)
HF_USE_LOCAL=1
LOCAL_MODEL=google/flan-t5-small

# PersistÃªncia/seed do Ã­ndice
INDEX_DIR=data
PERSIST_INDEX=1
AUTO_SEED=1
```

> Se quiser **apenas local**, Ã© suficiente `HF_USE_LOCAL=1` e `LOCAL_MODEL=google/flan-t5-small` (ou outro leve).

### 4) Subir o backend
```bash
uvicorn app.main:app --reload --port 8000
```
- Swagger: http://127.0.0.1:8000/docs  
- Health: `GET /health`

---

## ğŸ–¥ï¸ ConfiguraÃ§Ã£o (Frontend)

### 1) Ambiente e dependÃªncias
No diretÃ³rio `frontend/`:

```bash
# Linux/macOS
python -m venv .venv
source .venv/bin/activate
pip install streamlit python-dotenv requests
```

```powershell
# Windows
python -m venv .venv
.venv\Scripts\Activate.ps1
pip install streamlit python-dotenv requests
```

### 2) `frontend/.env` (exemplo)
```env
BACKEND_URL=http://127.0.0.1:8000
CHAT_DEFAULT_TOP_K=3
CHAT_DEFAULT_TEMPERATURE=0.2
CHAT_DEFAULT_MAX_NEW_TOKENS=120
```

### 3) Rodar o front
```bash
streamlit run frontend/app.py
```

---

## ğŸ”Œ API â€” VisÃ£o geral das rotas

### SaÃºde & Debug
- `GET /health` â†’ `{"status":"ok", "docs": <int>, "index_built": <bool>}`
- `GET /debug/config` â†’ mostra o que a API carregou do `.env` (oculta o token)
- `GET /debug/hf` â†’ **teste do LLM atual** (respeita fallback/local)
- `GET /debug/hf-remote` â†’ forÃ§a **Inference API** (sem fallback) â€” Ãºtil para diagnosticar

### IngestÃ£o
- `POST /ingest/sample`  
  Ingesta 3 textos de exemplo.
- `POST /ingest/texts`  
  **Body**:
  ```json
  { "texts": ["texto 1", "texto 2"], "chunk": true }
  ```
- `POST /ingest/file`  
  **Multipart**: `file=<.txt>`, `chunk=true|false`

### RAG â€œsimplesâ€
- `POST /query`  
  **Body**:
  ```json
  {
    "question": "O que Ã© RAG?",
    "top_k": 3,
    "temperature": 0.2,
    "max_new_tokens": 128
  }
  ```

### Chat (com histÃ³rico)
- `POST /chat`  
  **Body**:
  ```json
  {
    "session_id": "demo",
    "message": "Explique RAG em 1 frase.",
    "top_k": 3,
    "temperature": 0.2,
    "max_new_tokens": 120,
    "system_prompt": "opcional (muda o tom da resposta)"
  }
  ```
  **Resposta**: `answer`, `sources` (ids dos docs usados), `meta` (metadados), `debug.prompt`.

> No frontend, os **chips** exibem as fontes: `Doc 0`, `Doc 1`, ...

---

## ğŸ§  Embeddings e relevÃ¢ncia (PT-BR)

- Embeddings definidos em `.env` via `EMBED_MODEL`.  
  Recomendado:  
  `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- O serviÃ§o normaliza os textos (casefold + acentos), reduzindo sensibilidade a **minÃºsculas/maiÃºsculas** (ex.: â€œbrasilâ€ vs â€œBrasilâ€).
- O limiar de similaridade (`MIN_SIM`, em `rag.py`) foi ajustado para PT.  
  Se notar respostas â€œnÃ£o seiâ€ com textos parecidos, **reduza um pouco** esse limiar.

---

## â–¶ï¸ Fluxo de uso

1. Suba o backend:
   ```bash
   uvicorn app.main:app --reload --port 8000
   ```
2. Suba o frontend:
   ```bash
   streamlit run frontend/app.py
   ```
3. Ingerir exemplos/textos e perguntar no chat.  
   As respostas citam as **fontes** (Doc X, Doc Y).

---

## ğŸ” Boas prÃ¡ticas

- Nunca comite `.env` (token HF).  
- Se expor publicamente, configure **auth/rate-limit/CORS**.

---

## ğŸ“Œ Roadmap (sugestÃµes)

- Listagem/persistÃªncia de documentos com metadados
- HistÃ³rico por `session_id`
- Suporte a E5 + re-ranker
- Streaming de tokens no chat
